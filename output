
Rewards of real env is  [16.442966319514635, 14.919727639231782, 16.114451942114698, 15.743115809692505, 16.223115670038922]
Average reward is  15.888675476118511


===  Cycle  0 


Epoch 1 step 10: train dist 0.774010, loss 1.538583
Epoch 1 step 10: eval dist 0.722593

Rewards of imitation is  [16.139636887973143, 14.688880367360237, 16.313119058187297, 16.391717441499097, 16.074021331463815]
Average reward is  15.921475017296718
Difference is  1.3674756296008292
Logging to /tmp/openai-2019-06-27-22-14-07-762830
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 10       |
| mean 100 episode reward | -76.8    |
| steps                   | 1.8e+03  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 20       |
| mean 100 episode reward | -5.8     |
| steps                   | 3.8e+03  |
--------------------------------------

Reward of agent is  [13.395362994159136, 12.595979820124027, 11.709611067223182, 13.651632280725012, 12.258806531781861]
Average reward of agent is  12.722278538802644
End Cycle


===  Cycle  1 


Epoch 0 step 10: train dist 0.668893, loss 1.367655
Epoch 0 step 10: eval dist 0.601930
Epoch 1 step 20: train dist 0.579082, loss 1.244310
Epoch 1 step 20: eval dist 0.535838

Rewards of imitation is  [14.914989603682937, 13.149471220161592, 14.903997449918194, 14.205914894946545, 15.037952999695092]
Average reward is  14.442465233680872
Difference is  0.9729101754754249
Loaded model from mle-models/agent-mle-v0-0.pkl
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 10       |
| mean 100 episode reward | 40.2     |
| steps                   | 1.8e+03  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 20       |
| mean 100 episode reward | 48.8     |
| steps                   | 3.8e+03  |
--------------------------------------

Reward of agent is  [13.395362994159136, 12.595979820124027, 11.709611067223182, 13.651632280725012, 12.258806531781861]
Average reward of agent is  12.722278538802644
End Cycle


===  Cycle  2 


Epoch 0 step 10: train dist 0.499622, loss 1.088193
Epoch 0 step 10: eval dist 0.437954
Epoch 0 step 20: train dist 0.433529, loss 1.048985
Epoch 0 step 20: eval dist 0.393839
Epoch 1 step 30: train dist 0.389227, loss 1.033069
Epoch 1 step 30: eval dist 0.349448
Epoch 1 step 40: train dist 0.347950, loss 1.014002
Epoch 1 step 40: eval dist 0.303580

Rewards of imitation is  [15.528087857881243, 13.65714153838226, 15.21371205169259, 14.615055851996237, 15.40481428317281]
Average reward is  14.883762316625027
Difference is  0.5895075535989763
Loaded model from mle-models/agent-mle-v0-1.pkl
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 10       |
| mean 100 episode reward | 37.2     |
| steps                   | 1.8e+03  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 20       |
| mean 100 episode reward | 48.8     |
| steps                   | 3.8e+03  |
--------------------------------------

Reward of agent is  [13.395362994159136, 12.595979820124027, 11.709611067223182, 13.651632280725012, 12.258806531781861]
Average reward of agent is  12.722278538802644
End Cycle


===  Cycle  3 


Epoch 0 step 10: train dist 0.303468, loss 0.974188
Epoch 0 step 10: eval dist 0.257653
Epoch 0 step 20: train dist 0.268604, loss 0.974603
Epoch 0 step 20: eval dist 0.230283
Epoch 1 step 30: train dist 0.244232, loss 0.964698
Epoch 1 step 30: eval dist 0.206768
Epoch 1 step 40: train dist 0.220652, loss 0.956709
Epoch 1 step 40: eval dist 0.183210
Epoch 1 step 50: train dist 0.199538, loss 0.952130
Epoch 1 step 50: eval dist 0.160657

Rewards of imitation is  [16.243233550716113, 14.694917756752229, 15.988039350713754, 15.542926604315998, 16.10769269065986]
Average reward is  15.715361990631589
Difference is  0.29767142363825594
Loaded model from mle-models/agent-mle-v0-2.pkl
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 10       |
| mean 100 episode reward | 35.4     |
| steps                   | 1.8e+03  |
--------------------------------------
--------------------------------------
| % time spent exploring  | 2        |
| episodes                | 20       |
| mean 100 episode reward | 48.5     |
| steps                   | 3.8e+03  |
--------------------------------------
